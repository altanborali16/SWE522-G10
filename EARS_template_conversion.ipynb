{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correct_grammar(string):\n",
    "#     tool = LanguageTool('en-US')\n",
    "#     matches = tool.check(string)\n",
    "#     return tool.correct(string, matches)\n",
    "def correct_grammar(string):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    print(\"Correct\")\n",
    "    matches = tool.check(string)\n",
    "    return language_tool_python.utils.correct(string, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from textacy import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_list_string(lst):\n",
    "    for i in range(len(lst)):\n",
    "        print(i+1,\":\",lst[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_past_sentence(sentence):\n",
    "    sent = list(nlp(sentence).sents)[0]\n",
    "    return (\n",
    "        sent.root.tag_ == \"VBD\" or\n",
    "        any(w.dep_ == \"aux\" and w.tag_ == \"VBD\" for w in sent.root.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_passive_voice(sentence):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    doc = nlp(sentence)    \n",
    "    passive_rule = [{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]\n",
    "    matcher.add('Passive',[passive_rule])\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_actors(desc):\n",
    "    doc1 = nlp(desc)\n",
    "    actor = []\n",
    "    for sent in doc1.sents:\n",
    "        subj = \"\"\n",
    "        try:\n",
    "            SBOV_ext = textacy.extract.subject_verb_object_triples(sent)\n",
    "            SBOV = list(SBOV_ext)\n",
    "            if SBOV:\n",
    "                for triple in SBOV:\n",
    "                    subj_tokens = triple[0]  # Subject tokens\n",
    "                    subj_text = ' '.join(token.text for token in subj_tokens)\n",
    "                    actor.append(subj_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "\n",
    "        for token in sent:\n",
    "            #--------------------------------actor identification--------------------------------\n",
    "            if token.tag_ == \"NN\" and token.dep_ == \"nsubj\" and subj == token.text:\n",
    "                if token.text.lower() not in actor:\n",
    "                    actor.append(token.text.lower())\n",
    "    return actor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_nsubj(sent):\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            return token.text.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_1(sent,EARS_template_1):         #WHEN ..system shall...\n",
    "    temp = list.copy(EARS_template_1)\n",
    "    \n",
    "    comma_idx = sent.text.find(\",\")\n",
    "    \n",
    "    temp[1] = sent.text[5:comma_idx]\n",
    "    \n",
    "    comma_flg = False\n",
    "    rem_str = \"\"\n",
    "    verb_flg = False\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.tag_ == \",\":\n",
    "            comma_flg = True\n",
    "        elif comma_flg:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "                verb_flg = True\n",
    "            elif verb_flg:\n",
    "                if token.text.lower() == \"system\":\n",
    "                    continue\n",
    "                else:\n",
    "                    rem_str += token.text\n",
    "                    rem_str += \" \"\n",
    "    \n",
    "    temp[3] += rem_str\n",
    "    \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_2(sent,EARS_template_2,actor):\n",
    "    temp = list.copy(EARS_template_2)\n",
    "        \n",
    "\n",
    "    then_idx = sent.text.find(\"then\")\n",
    "    then_next = then_idx+5\n",
    "\n",
    "    if then_idx == -1:             #if then not found\n",
    "        then_idx = sent.text.find(\",\")\n",
    "        then_next = then_idx+1\n",
    "\n",
    "    if sent.text.startswith(\"If\"):    \n",
    "        temp[1] = sent.text[3:then_idx]\n",
    "    else:\n",
    "        temp[1] = sent.text[:then_idx]\n",
    "\n",
    "\n",
    "    verb_string = str(\"\")\n",
    "    temp[3] = \"\"\n",
    "    idx_rest_of_sent = 0\n",
    "    idx_aux = 0\n",
    "    flg = False\n",
    "    first_flg = False\n",
    "    \n",
    "    #without then statements remaining\n",
    "    \n",
    "    for i in range(len(sent)):\n",
    "        if sent[i].text == \"then\" and sent[i].dep_ == \"advmod\":\n",
    "            temp[3] += str(sent[i].head.lemma_)\n",
    "            verb_string = sent[i].head.text\n",
    "            flg = True\n",
    "            \n",
    "        elif flg:\n",
    "            if sent[i].pos_.lower() == \"aux\" and first_flg == False:\n",
    "                first_flg = True\n",
    "                \n",
    "            elif sent[i].text == \"system\":\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                if sent[i].text != verb_string:\n",
    "                    if sent[i].tag_ == \"PRP\":\n",
    "                        if actor[0] != \"system\":\n",
    "                            temp[3] += str(actor[0])\n",
    "                            temp[3] += \" \"\n",
    "                        else:\n",
    "                            temp[3] += str(actor[1])\n",
    "                            temp[3] += \" \"\n",
    "                    else:\n",
    "                        temp[3] += str(sent[i].text)\n",
    "                        temp[3] +=\" \"\n",
    "        \n",
    "            \n",
    "    if temp[1] == \"\" or temp[3] == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EARS_template_3(sent,EARS_template_3):\n",
    "    temp = list.copy(EARS_template_3)    #while template\n",
    "    \n",
    "    comma_idx = sent.text.find(\",\")\n",
    "    \n",
    "    temp[1] = sent.text[6:comma_idx]\n",
    "    \n",
    "    comma_flg = False\n",
    "    rem_str = \"\"\n",
    "    verb_flg = False\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.tag_ == \",\":\n",
    "            comma_flg = True\n",
    "        elif comma_flg:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "                verb_flg = True\n",
    "            elif verb_flg:\n",
    "                if token.text.lower() == \"system\":\n",
    "                    continue\n",
    "                else:\n",
    "                    rem_str += token.text\n",
    "                    rem_str += \" \"\n",
    "    \n",
    "    temp[3] += rem_str\n",
    "    \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Rupp_template_2(sent,Rupp_template_2,actor):\n",
    "    temp = list.copy(Rupp_template_2)      #change index 1-customer 3-PROCESS\n",
    "    subj_flg = False\n",
    "    verb_flg = False\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\" and token.text.lower() in actor:\n",
    "            subj_flg = True\n",
    "            temp[1] = token.text\n",
    "        \n",
    "        elif subj_flg == True:\n",
    "            if verb_flg == False and token.pos_.lower() == \"verb\":\n",
    "                verb_flg = True\n",
    "                temp[3] += token.lemma_\n",
    "                temp[3] += \" \"\n",
    "            else:\n",
    "                if token.tag_ == \"PRP\" and (token.text == \"he\" or token.text == \"she\"):\n",
    "                    if actor[0] != \"system\":\n",
    "                        temp[3] += str(actor[0])\n",
    "                        temp[3] += \" \"\n",
    "                    else:\n",
    "                        temp[3] += str(actor[1])\n",
    "                        temp[3] += \" \"\n",
    "                else:\n",
    "                    temp[3] += token.text\n",
    "                    temp[3] += \" \"\n",
    "                \n",
    "    if temp[1] != \"\" and temp[3] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Rupp_template_3(sent,Rupp_template_3,actor):\n",
    "    temp = list.copy(Rupp_template_3)      #change index 1-PROCESS\n",
    "    subj_flg = False\n",
    "    verb_flg = False\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"nsubj\" and token.text.lower() in actor:\n",
    "            subj_flg = True\n",
    "        \n",
    "        elif subj_flg == True:\n",
    "            if verb_flg == False and token.pos_.lower() == \"verb\":\n",
    "                verb_flg = True\n",
    "                temp[1] += token.lemma_\n",
    "                temp[1] += \" \"\n",
    "            else:\n",
    "                temp[1] += token.text\n",
    "                temp[1] += \" \"\n",
    "    \n",
    "    if temp[1] != \"\":\n",
    "        return correct_grammar(\"\".join(temp))\n",
    "    else:\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import preprocessing\n",
    "# from textacy.preprocessing import normalize_whitespace, preprocess_text\n",
    "def convert_requirements(desc,req):\n",
    "    # req = preprocessing.normalize_whitespace(req) #removing extra spaces\n",
    "    req = preprocessing.normalize.whitespace(req)\n",
    "    doc = nlp(req)\n",
    "\n",
    "    pre_cond = []\n",
    "    actor_action = []\n",
    "    condition = []\n",
    "    response = []\n",
    "\n",
    "    # desc = preprocessing.normalize_whitespace(desc)\n",
    "    desc = preprocessing.normalize.whitespace(desc)\n",
    "\n",
    "    actor = identify_actors(desc)\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        md_flag = False\n",
    "        admod = False\n",
    "        mark = False\n",
    "        cond = False\n",
    "        sys_flg = False\n",
    "\n",
    "\n",
    "        for token in sent:\n",
    "\n",
    "            #---------------------------------actor action---------------------------------------\n",
    "            if token.text in actor and detect_passive_voice(sent.text) == False and detect_past_sentence(sent.text) == False and sent not in actor_action:\n",
    "                actor_action.append(sent)\n",
    "\n",
    "            #--------------------------------precondition------------------------------------------    \n",
    "            if token.tag_ == \"MD\":\n",
    "                md_flag = True\n",
    "\n",
    "            if md_flag == True and sent not in pre_cond:\n",
    "                if detect_past_sentence(sent.text) == True:\n",
    "                    if token.tag_ == \"VBN\":\n",
    "                        pre_cond.append(sent)\n",
    "                else:\n",
    "                    pre_cond.append(sent)\n",
    "\n",
    "            #------------------------------conditional-----------------------------------------------\n",
    "            if token.dep_ == \"advmod\":\n",
    "                admod = True\n",
    "\n",
    "            if token.dep_ == \"mark\":\n",
    "                mark = True\n",
    "\n",
    "            if cond == False:\n",
    "                if admod == True and mark == True:\n",
    "                    condition.append(sent)\n",
    "                    cond = True\n",
    "\n",
    "            #------------------------------response-------------------------------------------------\n",
    "            if token.text.lower() == \"system\":\n",
    "                sys_flg = True\n",
    "\n",
    "            if sys_flg == True and token.text.lower() in actor and detect_past_sentence(sent.text) == False and sent not in response:\n",
    "                response.append(sent)\n",
    "\n",
    "\n",
    "    print(\"---------------ACTORS--------------------\")\n",
    "    print_list_string(actor)\n",
    "    print(\"---------------PRE-CONDITION--------------------\")\n",
    "    print_list_string(pre_cond)\n",
    "    print(\"---------------CONDITION--------------------\")\n",
    "    print_list_string(condition)\n",
    "    print(\"---------------RESPONSE--------------------\")\n",
    "    print_list_string(response)\n",
    "    print(\"---------------ACTOR_ACTION--------------------\")\n",
    "    print_list_string(actor_action)\n",
    "\n",
    "    #-----------------------------template structures--------------------------------\n",
    "    Rupp_template_1 = [\"The System shall \",\"\"]                               #replace index 1(PROCESS)\n",
    "    Rupp_template_2 = [\"The System shall provide \",\"\",\"the ability to \",\"\"]  #replace index 1(ACTOR) and 3(PROCESS)\n",
    "    Rupp_template_3 = [\"The System shall be able to \",\"\"]                    #replace index 1(PROCESS)\n",
    "\n",
    "    EARS_template_1 = [\"When \",\"\",\"the system shall \",\"\"]                     #replace index 1(PRE-COND||TRIGGER) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_2 = [\"If \",\"\",\"then the system shall \",\"\"]                  #replace index 1(PRE-COND||TRIGGER) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_3 = [\"While \",\"\",\"the system shall \",\"\"]                    #replace index 1(SYSTEM STATE) and 3(SYSTEM RESPONSE)\n",
    "    EARS_template_4 = [\"Where \",\"\",\"the system shall \",\"\"]                    #replace index 1(FEATURE) and 3(SYSTEM RESPONSE)\n",
    "\n",
    "    #------------------------second pass-----------------------\n",
    "    results = []\n",
    "    start_char = []\n",
    "    end_char = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        #--------------conditoinal--------------\n",
    "        if sent in condition:\n",
    "            res = make_EARS_template_2(sent,EARS_template_2,actor)\n",
    "            if res != \"\":\n",
    "                results.append(res)\n",
    "                start_char.append(sent.start_char)\n",
    "                end_char.append(sent.end_char)\n",
    "\n",
    "        #--------------actor_actional---------------\n",
    "        elif sent in actor_action:\n",
    "            if identify_nsubj(sent) == \"system\":\n",
    "                res = make_Rupp_template_3(sent,Rupp_template_3,actor) \n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "            else:\n",
    "                res = make_Rupp_template_2(sent,Rupp_template_2,actor)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "\n",
    "        else:\n",
    "            if sent.text.startswith(\"When\"):\n",
    "                res = make_EARS_template_1(sent,EARS_template_1)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "            elif sent.text.startswith(\"While\"):\n",
    "                res = make_EARS_template_3(sent,EARS_template_3)\n",
    "                if res != \"\":\n",
    "                    results.append(res)\n",
    "                    start_char.append(sent.start_char)\n",
    "                    end_char.append(sent.end_char)\n",
    "\n",
    "    \n",
    "    return results, start_char, end_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Quality-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\u25o52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\u25o52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\u25o52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# import tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text\n",
    "\n",
    "def sentence_similarity(sent1,sent2):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sent1 = [token.lower() for token in sent1]\n",
    "    sent2 = [token.lower() for token in sent2]\n",
    "    \n",
    "    all_words = list(set(sent1+sent2))\n",
    "    \n",
    "    vector1 = [0]*len(all_words)\n",
    "    vector2 = [0]*len(all_words)\n",
    "    \n",
    "    for w in sent1:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector1[all_words.index(w)]+=1\n",
    "    \n",
    "    for w in sent2:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        vector2[all_words.index(w)]+=1\n",
    "    return 1 - cosine_distance(vector1,vector2)\n",
    "    \n",
    "\n",
    "def get_duplication_index(sentences):\n",
    "    count = 0\n",
    "    global len_sent\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i+1,len(sentences)):\n",
    "            if sentence_similarity(sentences[i],sentences[j]) > 0.99:\n",
    "                count+=1\n",
    "                print(\"Number :\",count)\n",
    "                print(\"Sent_index\",i,\":\",sentences[i])\n",
    "                print(\"Sent_index\",j,\":\",sentences[j])\n",
    "                \n",
    "    return count/len_sent\n",
    "#     print(\"Duplication index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "\n",
    "def get_wordliness_index(requirement_text,total_sentences):\n",
    "    count = 0\n",
    "    tokens = nltk.word_tokenize(requirement_text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for i in tagged:\n",
    "        if i[1] == 'RB':\n",
    "            if i[0] != 'then' and i[0] != 'not' and i[0] != 'also':\n",
    "                print(i[0])\n",
    "#                 print(tokenizations.get_original_spans(tokens, requirement_text))\n",
    "                count+=1\n",
    "    return count/total_sentences\n",
    "#     print(\"Worliness index :\"+\"{:.2f}\".format(count/total_sentences))\n",
    "\n",
    "def get_ambiguity_index(sentences):\n",
    "    global len_sent\n",
    "    chunkGram = \"\"\"\n",
    "                    analytical: \n",
    "                        {<JJ><NN.*><NN.*>}\n",
    "                        \n",
    "                    coordination: \n",
    "                        {<JJ><NN.*><CC><NN.*>}\n",
    "                        \n",
    "                    PPAttachment: \n",
    "                        {<VB.*><DT>?<JJ>*<NN.*><IN><DT>?<JJ>*<NN.*>}\n",
    "            \"\"\"\n",
    "    count = 0\n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        result = chunkParser.parse(tagged)\n",
    "        \n",
    "        chunked_terms = []\n",
    "        for e in result.subtrees(filter=lambda t: t.label() == 'analytical' or t.label() == 'coordination' or t.label() == 'PPAttachment'):\n",
    "            if isinstance(e, tuple):\n",
    "                chunked_terms.append([ e[0] ])\n",
    "            else:\n",
    "                chunked_terms.append([w for w, t in e])\n",
    "                count+=1\n",
    "                print(chunked_terms)\n",
    "    return count/len_sent\n",
    "#     print(\"Ambiguity index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "        \n",
    "\n",
    "def get_vagueness_index(sentences):\n",
    "    dictionary = []\n",
    "    global len_sent\n",
    "    with open(\"Vagueness_Dictionary\") as f:\n",
    "        dictionary = f.read().splitlines()\n",
    "    count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for token in dictionary:\n",
    "            if token in sent:\n",
    "                print(token)\n",
    "                count+=1\n",
    "    #modification required for considering POS tags\n",
    "    return count/len_sent\n",
    "#     print(\"Vagueness index :\"+\"{:.2f}\".format(count/len(sentences)))\n",
    "\n",
    "def get_readability_index(text):\n",
    "    \n",
    "    nc = len(text) \n",
    "    words = word_tokenize(text)\n",
    "    nw = len(words)\n",
    "    sentences = sent_tokenize(text)\n",
    "    ns = len(sentences)\n",
    "    \n",
    "    #NC is the number of character in the text, NW the number of words, and NS the number of sentences\n",
    "    ig = 0\n",
    "    if not((nc == 0 and nw == 0) or (ns == 0 and nw == 0)):\n",
    "        ig =  89 - 10*nc/nw + 300*ns/nw         #Gulpease Index IG\n",
    "    \n",
    "    chunkGram = r\"\"\"NP: \n",
    "                        {<DT>?<JJ>*<NN>+}\n",
    "                        \n",
    "                    VP: \n",
    "                        {<PRP>?<VB|VBD|VBZ|VBG>*<RB|RBR>?}\n",
    "                    \n",
    "                    ADJP:\n",
    "                        {<RB|RBR>?<JJ>+}\n",
    "                    \n",
    "                    ABVP:\n",
    "                        {<RB|RBR><RB|RBR>+}\n",
    "                    \n",
    "                    PP:\n",
    "                        {<IN><DT>*<NN>}\n",
    "    \"\"\"\n",
    "    nk = 0         #number of chunks\n",
    "    \n",
    "    #initializing each chunk phrase counter\n",
    "    NP_count = 0\n",
    "    VP_count = 0\n",
    "    ADVP_count = 0\n",
    "    ADJP_count = 0\n",
    "    PP_count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        result = chunkParser.parse(tagged)\n",
    "        \n",
    "        NP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'NP')))\n",
    "        VP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'VP')))\n",
    "        ADVP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'ADVP')))\n",
    "        ADJP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'ADJP')))\n",
    "        PP_count += len(list(result.subtrees(filter=lambda t: t.label() == 'PP')))\n",
    "        \n",
    "        nk += (NP_count+VP_count+ADVP_count+ADJP_count+PP_count)\n",
    "                       \n",
    "    ik = 0        #Chunk Index Ik\n",
    "    if nk > 1:\n",
    "        ik = 100/(nk/len(sentences) - 1)\n",
    "    else:\n",
    "        ik = 100\n",
    "    \n",
    "    it = 0        #Chunk Type Index IT\n",
    "    \n",
    "    weighted_count = (NP_count*0.2691 + VP_count*0.4454 + ADJP_count*0.0379 + ADVP_count*0.0317 + PP_count*0.2159)\n",
    "    \n",
    "    if nk > 1:\n",
    "        it = 544*weighted_count/nk\n",
    "    else:\n",
    "        it = 100*weighted_count/0.2468\n",
    "    \n",
    "    print(\"Ig :\",ig,\"Ik :\",ik,\"It :\",it)\n",
    "    \n",
    "    readability_index = (ig + ik + it)/3\n",
    "    return 1 - readability_index/100\n",
    "#     print(\"Complexity index :\"+\"{:.2f}\".format(1 - readability_index/100))\n",
    "    \n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def get_understandability_index(text):\n",
    "    all_words = word_tokenize(text)\n",
    "    words = [w for w in all_words if w not in stop_words]\n",
    "#     ps = PorterStemmer()\n",
    "#     for i in range(len(words)):\n",
    "#         words[i] = ps.stem(words[i])\n",
    "    \n",
    "    word_dictionary = []\n",
    "    with open(\"Basic_english_words\") as f:\n",
    "        dictionary = f.read().splitlines()\n",
    "    nl = 0          #least used words\n",
    "    nb = len(all_words)-len(words)          #basic words\n",
    "    for word in words:\n",
    "        if word in word_dictionary:\n",
    "            nb+=1\n",
    "        else:\n",
    "            nl+=1\n",
    "    print(\"Nb:\",nb,\"Nl:\",nl)\n",
    "    iu = 0\n",
    "    if not(nb == 0 and nl == 0):\n",
    "        iu = (nb + 0.5*nl)/len(all_words)\n",
    "\n",
    "    \n",
    "    return 1-iu\n",
    "\n",
    "def get_quality_metrics(text_str):\n",
    "#     text_str = read_file(\"Point Of Sale System-PS.txt\")\n",
    "    global len_sent\n",
    "    sentences = sent_tokenize(text_str)\n",
    "    if len_sent == 0:\n",
    "        len_sent = len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = \"\".join([j.lower() for j in sentences[i] if j not in string.punctuation])\n",
    "\n",
    "    result_metrics = []\n",
    "    #calling metrics functions\n",
    "    print(\"-------------Duplication Index-----------\")\n",
    "    result_metrics.append(get_duplication_index(sentences))\n",
    "    print(\"-------------Wordliness Index-----------\")\n",
    "    result_metrics.append(get_wordliness_index(text_str,len_sent))\n",
    "    print(\"-------------Ambiguity Index-----------\")\n",
    "    result_metrics.append(get_ambiguity_index(sentences))\n",
    "    print(\"-------------Vagueness Index-----------\")\n",
    "    result_metrics.append(get_vagueness_index(sentences))\n",
    "    print(\"-------------Readability Index-----------\")\n",
    "    result_metrics.append(get_readability_index(text_str))\n",
    "    print(\"-------------Understandability Index-----------\")\n",
    "    result_metrics.append(get_understandability_index(text_str))\n",
    "    \n",
    "    return result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------ACTORS--------------------\n",
      "1 : Library Management System\n",
      "\n",
      "2 : user\n",
      "\n",
      "3 : Library Management System\n",
      "\n",
      "4 : system\n",
      "\n",
      "5 : details\n",
      "\n",
      "6 : user\n",
      "\n",
      "7 : user\n",
      "\n",
      "8 : system\n",
      "\n",
      "9 : user\n",
      "\n",
      "10 : user\n",
      "\n",
      "11 : librarian\n",
      "\n",
      "12 : user\n",
      "\n",
      "13 : system\n",
      "\n",
      "14 : member\n",
      "\n",
      "15 : librarian\n",
      "\n",
      "---------------PRE-CONDITION--------------------\n",
      "1 : Member must be logged in to the system.\n",
      "\n",
      "2 : The books should be stored in database.\n",
      "\n",
      "3 : The books must be ready to retrieve.\n",
      "\n",
      "4 : If User does not want to print the details then user can ignore the step.\n",
      "\n",
      "5 : User can reserve a book by inputting the relevant details & the librarian can also reserve a book for a member.\n",
      "\n",
      "6 : User should be logged into the system.\n",
      "\n",
      "7 : User should have correct book Id. Books should be available to reserve.\n",
      "\n",
      "8 : Members should be logged into the system.\n",
      "\n",
      "9 : Guest user can also search books.\n",
      "\n",
      "10 : Book should be available to search.\n",
      "\n",
      "11 : Member should give the member Id to the librarian.\n",
      "\n",
      "12 : Books should be available to issue.\n",
      "\n",
      "13 : The librarian cannot issue the books if the user has three books issued on his Id.\n",
      "\n",
      "14 : Librarian should be logged into the system.\n",
      "\n",
      "15 : Member should have borrowed books.\n",
      "\n",
      "16 : Member should give the member Id to the librarian.\n",
      "\n",
      "17 : Members should be stored in the database.\n",
      "\n",
      "18 : Members should be available to retrieve by the system.\n",
      "\n",
      "19 : Member must be logged in to the system.\n",
      "\n",
      "20 : Guest can also view members.\n",
      "\n",
      "21 : Librarian should be logged into the system.\n",
      "\n",
      "22 : Books should be available to remove.\n",
      "\n",
      "23 : Book details should be available to add or remove books in the database.\n",
      "\n",
      "24 : If the librarian selects to remove a book then the book should be outdated.\n",
      "\n",
      "25 : Administrator should be logged into the system.\n",
      "\n",
      "26 : Member should be available to remove.\n",
      "\n",
      "27 : Member details should be available to add or remove member in the database.\n",
      "\n",
      "---------------CONDITION--------------------\n",
      "1 : If the entered information is wrong then system asks member to reenter the details.\n",
      "\n",
      "2 : If no category is selected by the user then the system again asks user to select a category.\n",
      "\n",
      "3 : If User does not want to print the details then user can ignore the step.\n",
      "\n",
      "4 : If no book is selected by the user then the system again asks user to select a book to reserve.\n",
      "\n",
      "5 : If the book Id is wrong then system asks user to recheck the book id.\n",
      "\n",
      "6 : If the selected book is already reserved on another Id then system asks user to select another book.\n",
      "\n",
      "7 : If no category is selected by the member then the system again asks user to select a category.\n",
      "\n",
      "8 : The librarian also checks total number of books issued on that Id. Librarian issues the book.\n",
      "\n",
      "9 : If the entered book id is incorrect then system asks to re-enter the book id.\n",
      "\n",
      "10 : The system prompts a message that the book with book id is successfully returned.\n",
      "\n",
      "11 : If the librarian selects to remove a book then the book should be outdated.\n",
      "\n",
      "12 : The system administrator that all the information has been correctly provided.\n",
      "\n",
      "13 : If the administrator selects to remove a member then a valid reason of removal is required.\n",
      "\n",
      "14 : The system validates that all the information has been correctly provided.\n",
      "\n",
      "---------------RESPONSE--------------------\n",
      "1 : The system displays the log-in page.\n",
      "\n",
      "2 : The system checks that all of the required information was entered.\n",
      "\n",
      "3 : If the entered information is wrong then system asks member to reenter the details.\n",
      "\n",
      "4 : The system validates the entered information against the tables stored in the database.\n",
      "\n",
      "5 : Member must be logged in to the system.\n",
      "\n",
      "6 : The system opens a page showing the details of the member.\n",
      "\n",
      "7 : The system shows OK message.\n",
      "\n",
      "8 : The system asks user to login.\n",
      "\n",
      "9 : The system identifies the type of user- member, guest or administrator.\n",
      "\n",
      "10 : The system shows the categories to browse.\n",
      "\n",
      "11 : If no category is selected by the user then the system again asks user to select a category.\n",
      "\n",
      "12 : The system checks the books in the database.\n",
      "\n",
      "13 : The system retrieves all the books falling in that category.\n",
      "\n",
      "14 : The system shows the details of the selected books.\n",
      "\n",
      "15 : The system asks user to print the details.\n",
      "\n",
      "16 : User should be logged into the system.\n",
      "\n",
      "17 : The system asks user to login.\n",
      "\n",
      "18 : The system identifies the type of user- member, guest or administrator.\n",
      "\n",
      "19 : The system shows the categories to browse.\n",
      "\n",
      "20 : If no book is selected by the user then the system again asks user to select a book to reserve.\n",
      "\n",
      "21 : If the book Id is wrong then system asks user to recheck the book id.\n",
      "\n",
      "22 : The system checks the books in the database.\n",
      "\n",
      "23 : If the selected book is already reserved on another Id then system asks user to select another book.\n",
      "\n",
      "24 : Members should be logged into the system.\n",
      "\n",
      "25 : The system shows the categories to browse.\n",
      "\n",
      "26 : If no category is selected by the member then the system again asks user to select a category.\n",
      "\n",
      "27 : The system checks the books in the database.\n",
      "\n",
      "28 : The system retrieves all the books falling in that category.\n",
      "\n",
      "29 : The system updates the information in database.\n",
      "\n",
      "30 : Librarian should be logged into the system.\n",
      "\n",
      "31 : The librarian enters book id , member id in the system.\n",
      "\n",
      "32 : If the entered book id is incorrect then system asks to re-enter the book id.\n",
      "\n",
      "33 : The system prompts a message that the book with book id is successfully returned.\n",
      "\n",
      "34 : Members should be available to retrieve by the system.\n",
      "\n",
      "35 : Member must be logged in to the system.\n",
      "\n",
      "36 : The system opens a page showing the details of the member.\n",
      "\n",
      "37 : Librarian should be logged into the system.\n",
      "\n",
      "38 : The system asks librarian to add or remove the book.\n",
      "\n",
      "39 : The system asks librarian to enter all the required details about the new book to be added.\n",
      "\n",
      "40 : The system administrator that all the information has been correctly provided.\n",
      "\n",
      "41 : Administrator should be logged into the system.\n",
      "\n",
      "42 : The system asks administrator to add or remove a member.\n",
      "\n",
      "43 : The system asks administrator to enter all the required details about the new member to be added.\n",
      "\n",
      "44 : The system validates that all the information has been correctly provided.\n",
      "\n",
      "---------------ACTOR_ACTION--------------------\n",
      "1 : The system displays the log-in page.\n",
      "\n",
      "2 : The member enters his login Id with password.\n",
      "\n",
      "3 : The member clicks on the confirm button.\n",
      "\n",
      "4 : The system checks that all of the required information was entered.\n",
      "\n",
      "5 : If the entered information is wrong then system asks member to reenter the details.\n",
      "\n",
      "6 : The system validates the entered information against the tables stored in the database.\n",
      "\n",
      "7 : The member clicks on View user details.\n",
      "\n",
      "8 : The system opens a page showing the details of the member.\n",
      "\n",
      "9 : The details include the total number of issued books, date of issue, return date, fine to be paid.\n",
      "\n",
      "10 : The member closes the page.\n",
      "\n",
      "11 : The system shows OK message.\n",
      "\n",
      "12 : The system asks user to login.\n",
      "\n",
      "13 : The system identifies the type of user- member, guest or administrator.\n",
      "\n",
      "14 : The system shows the categories to browse.\n",
      "\n",
      "15 : The user selects a category of books to view.\n",
      "\n",
      "16 : The system checks the books in the database.\n",
      "\n",
      "17 : The system retrieves all the books falling in that category.\n",
      "\n",
      "18 : The user selects the desired books.\n",
      "\n",
      "19 : The system shows the details of the selected books.\n",
      "\n",
      "20 : The system asks user to print the details.\n",
      "\n",
      "21 : If User does not want to print the details then user can ignore the step.\n",
      "\n",
      "22 : User can reserve a book by inputting the relevant details & the librarian can also reserve a book for a member.\n",
      "\n",
      "23 : The system asks user to login.\n",
      "\n",
      "24 : The system identifies the type of user- member, guest or administrator.\n",
      "\n",
      "25 : The system shows the categories to browse.\n",
      "\n",
      "26 : The user selects book to reserve.\n",
      "\n",
      "27 : The user enters Book Id to reserve.\n",
      "\n",
      "28 : If the book Id is wrong then system asks user to recheck the book id.\n",
      "\n",
      "29 : The system checks the books in the database.\n",
      "\n",
      "30 : If the selected book is already reserved on another Id then system asks user to select another book.\n",
      "\n",
      "31 : Guest user can also search books.\n",
      "\n",
      "32 : The system shows the categories to browse.\n",
      "\n",
      "33 : The member selects a category of searching a book.\n",
      "\n",
      "34 : The system checks the books in the database.\n",
      "\n",
      "35 : The system retrieves all the books falling in that category.\n",
      "\n",
      "36 : Member should give the member Id to the librarian.\n",
      "\n",
      "37 : The librarian Checks the availability of the books.\n",
      "\n",
      "38 : The librarian also checks total number of books issued on that Id. Librarian issues the book.\n",
      "\n",
      "39 : The librarian cannot issue the books if the user has three books issued on his Id.\n",
      "\n",
      "40 : The system updates the information in database.\n",
      "\n",
      "41 : Member should give the member Id to the librarian.\n",
      "\n",
      "42 : The member returns the book.\n",
      "\n",
      "43 : The librarian enters book id , member id in the system.\n",
      "\n",
      "44 : If the entered book id is incorrect then system asks to re-enter the book id.\n",
      "\n",
      "45 : The system prompts a message that the book with book id is successfully returned.\n",
      "\n",
      "46 : Members should be available to retrieve by the system.\n",
      "\n",
      "47 : The user clicks on View members.\n",
      "\n",
      "48 : The system opens a page showing the details of the member.\n",
      "\n",
      "49 : The details include name of member, the total number of issued books, date of issue, return date, fine to be paid.\n",
      "\n",
      "50 : The member closes the page.\n",
      "\n",
      "51 : Book details should be available to add or remove books in the database.\n",
      "\n",
      "52 : The librarian has option of adding or removing a book in database.\n",
      "\n",
      "53 : The system asks librarian to add or remove the book.\n",
      "\n",
      "54 : The librarian adds a book.\n",
      "\n",
      "55 : The system asks librarian to enter all the required details about the new book to be added.\n",
      "\n",
      "56 : The librarian enters the details.\n",
      "\n",
      "57 : The system administrator that all the information has been correctly provided.\n",
      "\n",
      "58 : Member details should be available to add or remove member in the database.\n",
      "\n",
      "59 : The system asks administrator to add or remove a member.\n",
      "\n",
      "60 : The administrator selects to add a member.\n",
      "\n",
      "61 : The system asks administrator to enter all the required details about the new member to be added.\n",
      "\n",
      "62 : The administrator enters the details.\n",
      "\n",
      "63 : If the administrator selects to remove a member then a valid reason of removal is required.\n",
      "\n",
      "64 : The system validates that all the information has been correctly provided.\n",
      "\n",
      "Correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py\", line 1885, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\u25o52\\AppData\\Local\\Temp\\ipykernel_8832\\173835817.py\", line 11, in getTextInput\n",
      "    template_list, start_char, end_char = convert_requirements(desc,req)\n",
      "  File \"C:\\Users\\u25o52\\AppData\\Local\\Temp\\ipykernel_8832\\2383217313.py\", line 101, in convert_requirements\n",
      "    res = make_Rupp_template_3(sent,Rupp_template_3,actor)\n",
      "  File \"C:\\Users\\u25o52\\AppData\\Local\\Temp\\ipykernel_8832\\1024907574.py\", line 19, in make_Rupp_template_3\n",
      "    return correct_grammar(\"\".join(temp))\n",
      "  File \"C:\\Users\\u25o52\\AppData\\Local\\Temp\\ipykernel_8832\\2442092847.py\", line 8, in correct_grammar\n",
      "    corrected_text = tool.correct(string)\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\language_tool_python\\server.py\", line 153, in correct\n",
      "    return correct(text, self.check(text))\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\language_tool_python\\server.py\", line 129, in check\n",
      "    response = self._query_server(url, self._create_params(text))\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\language_tool_python\\server.py\", line 218, in _query_server\n",
      "    with requests.get(url, params=params, timeout=self._TIMEOUT) as response:\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 790, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 461, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"c:\\Users\\u25o52\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "\n",
    "def go(event,start_char,end_char):\n",
    "    cs = outputtext.curselection()\n",
    "    tbox2.tag_add(\"highlight\", \"1.0+\"+str(start_char[cs[0]])+\"c\", \"1.0+\"+str(end_char[cs[0]])+\"c\") \n",
    "    tbox2.tag_config(\"highlight\", background=\"yellow\", foreground=\"black\")\n",
    "    \n",
    "def getTextInput():\n",
    "    desc=tbox1.get(\"1.0\",\"end\")\n",
    "    req=tbox2.get(\"1.0\",\"end\")\n",
    "    template_list, start_char, end_char = convert_requirements(desc,req)\n",
    "    template_string = \"\"\n",
    "    print(\"template list : \" , template_list)\n",
    "    for i in range(len(template_list)):\n",
    "        template = template_list[i] if template_list[i] is not None else ''  # Check if template_list[i] is None\n",
    "        outputtext.insert(i, str(i + 1) + \". \" + str(template) + \"\\n\\n\")\n",
    "        if template:\n",
    "            template_string += template\n",
    "    \n",
    "    outputtext.bind('<Double-1>', lambda event, arg1=start_char, arg2 = end_char: go(event, arg1, arg2))\n",
    "    \n",
    "    \n",
    "    text_str= desc + req\n",
    "    output_metrics = get_quality_metrics(text_str)\n",
    "    output_string = \"Duplication index: \"+\"{:.2f}\".format(output_metrics[0])+\"\\n\"+\"Wordliness index: \"+\"{:.2f}\".format(output_metrics[1])+\"\\n\"+\"Ambiguity index: \"+\"{:.2f}\".format(output_metrics[2])+\"\\n\"+\"Vagueness index: \"+\"{:.2f}\".format(output_metrics[3])+\"\\n\"+\"Redability index: \"+\"{:.2f}\".format(output_metrics[4])+\"\\n\"+\"Understandability index: \"+\"{:.2f}\".format(output_metrics[5])\n",
    "    outputtext1.insert(END, output_string)\n",
    "    \n",
    "    print(template_string)\n",
    "    \n",
    "    output_metrics1 = get_quality_metrics(template_string)\n",
    "    output_string1 = \"Duplication index: \"+\"{:.2f}\".format(output_metrics1[0])+\"\\n\"+\"Wordliness index: \"+\"{:.2f}\".format(output_metrics1[1])+\"\\n\"+\"Ambiguity index: \"+\"{:.2f}\".format(output_metrics1[2])+\"\\n\"+\"Vagueness index: \"+\"{:.2f}\".format(output_metrics1[3])+\"\\n\"+\"Redability index: \"+\"{:.2f}\".format(output_metrics1[4])+\"\\n\"+\"Understandability index: \"+\"{:.2f}\".format(output_metrics1[5])\n",
    "    outputtext2.insert(END, output_string1)\n",
    "    \n",
    "    # for i in range(len(template_list)):\n",
    "    #     template_string += str(i+1) + \". \" + template_list[i] + \"\\n\\n\"\n",
    "    # outputtext.insert(END, template_string)\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Automated conversion to EARS/Rupp's template\")\n",
    "frame=Frame(root, width=1300, height=600)\n",
    "frame.pack()\n",
    "\n",
    "len_sent = 0\n",
    "\n",
    "text1 = Label(text=\"Requirement description:\")\n",
    "text1.place(x=10, y=10)\n",
    "\n",
    "tbox1 = Text(frame,font=('Times', 14))\n",
    "tbox1.place(x=10, y=40, height=150, width=300)\n",
    "\n",
    "text2 = Label(text=\"Requirement Textual Specification:\")\n",
    "text2.place(x=10, y=200)\n",
    "\n",
    "tbox2 = Text(frame,font=('Times', 14))\n",
    "tbox2.place(x=10, y=225, height=360, width=300)\n",
    "\n",
    "button1 = Button(frame, text=\"Convert Requirements\", command = getTextInput)\n",
    "button1.place(x=320, y=300, height=50, width=125)\n",
    "\n",
    "\n",
    "\n",
    "# outputtext = Text(frame)\n",
    "# outputtext.place(x=450, y = 40, height=540, width=500)\n",
    "\n",
    "text3 = Label(text=\"Converted Requirements to EARS/Rupp's template format:\")\n",
    "text3.place(x=450, y=10)\n",
    "\n",
    "outputtext = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext.place(x=450, y = 40, height=360, width=800)\n",
    "\n",
    "text4 = Label(text=\"Quality metrics before conversion:\")\n",
    "text4.place(x=450, y=420)\n",
    "\n",
    "# outputtext1 = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext1 = Text(frame,height=10, font=('Times', 14))\n",
    "outputtext1.place(x=450, y = 450, height=130, width=350)\n",
    "\n",
    "text5 = Label(text=\"Quality metrics after conversion:\")\n",
    "text5.place(x=900, y=420)\n",
    "\n",
    "# outputtext1 = Listbox(frame,height=10, font=('Times', 14))\n",
    "outputtext2 = Text(frame,height=10, font=('Times', 14))\n",
    "outputtext2.place(x=900, y = 450, height=130, width=350)\n",
    "\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
